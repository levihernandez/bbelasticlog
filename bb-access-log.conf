###################################################################
# Title	        : Blackboard Access Log
# Description.  : Parse logs, mutate values, split kv pairs, extract params
# Author        : Julian Levi Hernandez
# Date          : 08/03/2016
# Aumersbee Data
###################################################################

# Decompress access log into inbound dir organized by day. The wildcard [*] will grab all dirs.
# /NFS/data/blackboard/inbound/20190401/bb-access*.txt
input {
    file {
        # Assign 'access' as a container for the data
        type => "access"
        # Path to dir where unzipped bb-access*.txt log resides
        path => "/NFS/data/blackboard/inbound/*/bb-access*.txt"
        # Delete the sincedb file if reprocessing the same file
        sincedb_path => "/NFS/data/blackboard/transform/access.sincedb"
        # position 'beginning' allows to reprocess files from the top, if a file was processed half way, logstash is smart enough to resume the parsing of the log where it left off
        start_position => "beginning"
        #codec => gzip_lines { charset => "ISO-8859-1"}
        # ignore_older => 8000
    }
}

filter {
  grok {
    match => ["path","%{GREEDYDATA}/%{GREEDYDATA:filename}\.txt"]

  }


  # Parses each access log line to split them into columns based on GROK patterns
    if [type] == "access" {
        grok {
            patterns_dir => ["./patterns"]
                        break_on_match => true
                        # Two versions of GROK are created to accommodate custom fields enabled in the tomcat xml conf
                        # The first GROK for 'message' contains more custom fields, the second is the out of stock BB log
                        match => { "message" => [ "%{IP:client_ip}\s%{IP:local}\s%{USERNAME:thread}\s%{USERNAME:user_pk}\s\[%{DATA:dte}:%{DATA:time}\]\s\"%{WORD:method}\s(?:%{URIPATHPARAM:uri_param})?\s%{DATA:httpversion}\"\s%{DATA:status_code}\s%{DATA:bytes}\s\"%{DATA:browser}\"\s(\"(%{DATA:drop}\s(\bsession_id=\b(?<session_id>.{0,32})\;)\s%{DATA:drops}\")|%{DATA:dro}\")\s%{DATA:seconds}\s%{DATA:bytesdup}\s\"%{DATA:unixsession}\"\s(\"%{URIPROTO:refprotocol}://(?:%{USER:user}(?::[^@]*)?@)?(?:%{URIHOST:refdomain})?(?:%{URIPATHPARAM:refparam})?\"|%{DATA:dro}\")\s\"%{DATA:querys}\"\s\"%{DATA:referrer}\"",
                        "%{IP:client_ip}\s%{IP:local}\s%{USERNAME:thread}\s%{USERNAME:user_pk}\s\[%{DATA:dte}:%{DATA:time}\]\s\"%{WORD:method}\s(?:%{URIPATHPARAM:uri_param})?\s%{DATA:httpversion}\"\s%{DATA:status_code}\s%{DATA:bytes}\s\"%{DATA:browser}\"\s(\"(%{DATA:drop}\s(\bsession_id=\b(?<session_id>.{0,32})\;)\s%{DATA:drops}\")|%{DATA:dro}\")\s%{DATA:msec}\s%{DATA:bytesdup}"]
                    }
            # Verify message line gets destroyed in memory
            overwrite => ["message"]
        }
        
        # parse timestamp into a new field called time
        grok {match => ["time","%{HOUR:loghour}:%{MINUTE:logminute}:%{SECOND:logsecond}\s%{ISO8601_TIMEZONE:logtimezone}"]}
        # parse the uri and split it into multiple columns
        grok {
            # (?:%{URIPATH:uripath})?(?:%{URIPARAM:refparam})?
        match => [ "uri_param", "(?:%{URIPATH:uri_path})?(?:%{URIPARAM:uriparam})?" ]
        }
        grok {
        match => [ "refparam", "(?:%{URIPATH:refpath})?(?:%{URIPARAM:refquery})?" ]
        }
        # separate the uri query into key, values. This is tricky because it will produce a dynamic number of columns. It is not necessary for generic querying of the Bb access log.
        kv {
        source => "refquery"
        field_split => "?&"
        target => "ref_query"
        }
        # Separate the parameters from the uri query and set them as key, value columns. The information is useful but generates a large number of columns. Which makes it difficult for a CSV file. JSON files can handle this better but are more difficult to query.
        kv {
        source => "uriparam"
        field_split => "?&"
        target => "uri_query"
        remove_char_key => "<>\[\],\."
        }
        # Parse the browser type and enrich the data
        useragent {
            add_tag => [ "UA" ]
            source => "browser"
        target => "agent"
        }
        # Remove unwanted columns, add new columns
        mutate {
            remove_field => "filename"
            remove_field => "message"
            #remove_field => "browser"
            remove_field => "dro"
            remove_field => "drop"
            remove_field => "drop1"
            remove_field => "drops"
            remove_field => "local"
            remove_field => "bytesdup"
            remove_field => "httpversion"
            #remove_field => "path"
            remove_field => "time"
            remove_field => "dte"
            remove_field => "uri_param"
            remove_field => "tags"
            add_field => {"dttime" => "%{dte} %{time}"
            }
        }
    }
    
    # Replace or trim values, set default values
    mutate {
        gsub => [
          # replace all dashes with zero
          "user_pk", "_1", ""
        ]
        gsub => [
          # replace all dashes with zero
          "user_pk", "_", ""
        ]
        gsub => [
          # replace all dashes with zero
          "user_pk", "-", "0"
        ]
        gsub => [
          "thread", "connector-", ""
        ]
        gsub => [
          # replace all dashes with zero
          "[uri_query][course_id]", "_1", ""
        ]
        gsub => [
          # replace all dashes with zero
          "[uri_query][course_id]", "_", ""
        ]
        gsub => [
          # replace all dashes with zero
          "[uri_query][content_id]", "_1", ""
        ]
        gsub => [
          # replace all dashes with zero
          "[uri_query][content_id]", "_", ""
        ]
        gsub => [
          # replace all dashes with zero
          "[uri_query][course_assessment_id]", "_1", ""
        ]
        gsub => [
          # replace all dashes with zero
          "[uri_query][course_assessment_id]", "_", ""
        ]
        #rename => {"@timestamp" => "timestamp" }
        rename => {"@version" => "version" }
      }

      # Enrich data with GeoIP
      # Get GeoIP information based on GeoIP Lite internal DB. A paid version exists with more accurate and detailed information
      # Internal IPs will not include informatio, unless ITS creates an inventory of IPs based on building and room location
      geoip {
          source => "client_ip"
      }
}


output {
## Uncomment line 'stdout' below to see output in screen, it is recommended to hide the if statements for CSV while testing
##  stdout { codec => rubydebug }

    # Define the parsed columns to save into the CSV file
    if [type] == "access" {
            csv_options => {"col_sep" => "|"}
            fields => ["client_ip", "user_pk", "dttime", "method", "uripath", "uriparam", "iquery", "status_code", "bytes", "browser","device", "os_name", "os_major","os_minor", "name", "major", "minor", "patch", "session_id", "seconds", "unixsession", "refprotocol", "refdomain", "refpath", "refquery", "rquery",  "referrer", "hostid", "geoip"]
            # Write CSV files every hour + minute, remove %{+mm} if files will be preserved by the hour. Depending on the logstash parsing speed, some files might be too large if procees per hour.
            path => "/NFS/data/blackboard/output/access/%{+YYYY.MM.dd}/%{+HH}_%{+mm}-access.csv"
        }
    }
}